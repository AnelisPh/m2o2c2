\begin{document}
	\section{Abstract Vector Spaces}
	
	Until now, we have only dealt with the spaces $\R^n$.  We now begin the journey of understanding differential calculus on slightly more 
	general spaces.  
	
	The crucial structure we need to talk about linear maps on $\R^n$ are addition and scalar multiplication.  Addition is a function which takes a 
	pair of vectors $v,w \in \R^n$ and returns a new vector $v+w \in \R^n$.  Scalar multiplication is function which takes a vector $v \in \R^n$  and 
	a scalar $c \in \R$, and returns a new vector $cv$.  
	
	\begin{definition}
		A \textit{Vector Space} is a set $V$ equipped with a notion of addition, which is a function that takes a pair of vectors $\vec{v},\vec{w} \in V$ and returns 
		a new vector $\vec{v}+\vec{w}$, and a notion of scalar multiplication, which is a function that takes a scalar $c \in \R$ and a vector $\vec{v} \in V$ and returns a 
		new vector $c\vec{v} \in V$.
		
		These operations are subject to the following requirements:
		
		\begin{itemize}
			\item (Commutativity) For each $\vec{v},\vec{w} \in V$, $ \vec{v} +\vec{w} = \vec{w}+\vec{v}$
			\item (Associativity) For each $\vec{v},\vec{w},\vec{u} \in V$, $\vec{v} +(\vec{w} +\vec{u}) = (\vec{v} +\vec{w}) +\vec{u}$
			\item (Additive identity) There is a vector called $\vec{0} \in V$ with $\vec{v}+\vec{0} = \vec{v}$ for each $\vec{v} \in V$.
			\item (Additive inverse) For each $\vec{v} \in V$ there is a vector $\vec{w} \in V$ with $v+w = \vec{0}$
			\item (Multiplicative identity) For each $\vec{v} \in V$, $1\vec{v}= \vec{v}$ (here $1$ is really the real number $1 \in \R$, and $1\vec{v}$ is the
			scalar product of $1$ with $v$)
			\item (Distributivity of scalar multiplication over vector addition)  For each $\vec{v},\vec{w} \in V$ and $c \in \R$, $c(\vec{v}+\vec{w}) = c\vec{v} + c\vec{w}$
			\item (Distributivity of vector  multiplication under scalar addition) For each $a,b \in \R$ and $\vec{v} \in V$, $(a+b)\vec{v} = a\vec{v}+b\vec{v}$
		\end{itemize}
		
	\end{definition}
	
	\begin{question}
		Verify that $\R^n$ is a vector space with the notions of vector addition and scalar multiplication we introduced in week 1.  
	\end{question}

	\begin{question}
		Let $Poly_2$ be the set of all polynomials of degree at most $2$ in one variable.  For example $1+2x \in Poly_2$ and $3+x^2 \in Poly_2$.  Then the usual way of 
		adding polynomials and multiplying them by constants turns $Poly_2$ into a vector space.  For example $2(1+2x)+(3+x^2)  = 7+4x+x^2$.
		
		Prove that $Poly_2$ is a vector space with these operations.
	\end{question}
	
	The next example will probably be the most important example for us.  Thinking of matrices as points in a vector space will be useful for us when we 
	start thinking about the second derivative.
	
	\begin{question}
		Let $Mat_{nm}$ be the collection of all $n \times m$ matrixes.  Show that $Mat_{nm}$ is a vector space with the usual notion of matrix addition and scalar multiplication.
	\end{question}
	
	The next example is an important, but very small, first step into the world of functional analysis:
	
	\begin{question}
		Let $\mathcal{C}([0,1])$ be the set of all continuous functions from $[0,1]$ to $\R$.  Show that $\mathcal{C}([0,1])$ is a vector space with addition 
		being the usual sum of functions, and scalar multiplication being the usual product of functions.
	\end{question}
	
	Realizing that solution sets of certain differential equations form vector spaces is really important.  
	
	\begin{question}
		Let $V$ be the set of all differentiable functions $f: \R \to \R$ which solve the differential equation $\frac{d^2f}{dx^2} + 3\frac{df}{dx} +4f(x) = 0$.
		Addition is the usual addition of functions, likewise for scalar multiplication.  Show that $V$ is a vector space.
		
		What is we change the differential equation to $\frac{d^2f}{dx^2} + 3\frac{df}{dx} +4 = 0$?  Is this still a vector space?  Why or why not?
	\end{question}
	
	We are dealing with a level of abstraction here that you are not likely to have met before.  It is worthwhile taking some time to prove certain ``obvious'' seeming
	statements formally from the axioms:
	
	\begin{question}
		Prove that in any vector space, there is a unique additive identity.  In other words, if $V$ is a vector space, and there are two elements $\vec{0},\vec{0'} \in V$ so that for each 
		$\vec{v} \in V$, $\vec{v}+\vec{0}=\vec{v}$ and $\vec{v} + \vec{0'} = \vec{v}$, then $0=0'$.
	\end{question}
	
	\begin{question}
		Prove that each element of a vector space has only one additive inverse.
	\end{question}
	
	\begin{question}
		Let $V$ be a vector space. Prove that $0\vec{v} = \vec{0}$ for every $\vec{v} \in V$. (Note: $0$ means different things on the different sides of the equation!  On
		the left hand side, it is the scalar $0\in\R$, whereas on the right hand side it is the zero vector $\vec{0} \in V$ )
	\end{question}
	
	\begin{question}
		Let $V$ be a vector space.  Prove that $a\vec{0} = \vec{0}$ for every $a \in \R$
	\end{question}
	
	\begin{question}
		Let $V$ be a vector space.  Prove that $(-1)\vec{v}$ is the additive inverse of $\vec{v}$ for every $\vec{v} \in \R$.
		
		\begin{answer}
			The proof of this uses the ``rat poison principle'':  if you want to show that something is rat poison, try feeding it to a rat! 
			In this case we want to see if $(-1)\vec{v}$ is the additive inverse of $\vec{v}$, so we should try adding it to $\vec{v}$.
			$(-1)\vec{v}+\vec{v} = (-1)\vec{v}+1\vec{v} = (-1+1)\vec{v} = 0\vec{v} = \vec{0}$, so, indeed, $(-1)\vec{v}$ is an additive inverse
			of $\vec{v}$.  We already proved uniqueness of additive inverses above, so we are done.  We will often simply write $-\vec{v}$ for the additive inverse of
			$\vec{v}$ in the future.
		\end{answer}
	\end{question}
	
\end{document}