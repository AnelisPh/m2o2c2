\documentclass{ximera}
\title{Adjoints}

\begin{document}

\begin{abstract}
  Adjoints formalize the transpose.
\end{abstract}
	
Let $L: \R^m \to \R^n$ be a linear map.  Then there is an associated
bilinear form $B_L : \R^n \times \R^m \to \R$ given by
$B_L(\vec{v},\vec{w}) = \langle \vec{v}, L(\vec{w}) \rangle$.

One thing we can do to a bilinear map is swap the two inputs, namely,
we can build $B_L^*: \R^m \times \R^n \to \R$ by the rule $B_L^*
(\vec{w},\vec{v}) = B_L(\vec{v},\vec{w})$.

And with this ``swapped'' bilinear map, we can go back and recover an
associated linear map $L_{B_L^*} : \R^n \to \R^m$.

\begin{question}
  The domain of $L_{B_L^*}$ is the same as
  \begin{solution}
    \begin{multiple-choice}
      \choice[correct]{the codomain of $L$.}
      \choice{the codomain of $L$.}
    \end{multiple-choice}
  \end{solution}

  Right!  The minor surprise is that $L$ went from $\R^m$ to $\R^n$, but $L_{B_L^*}$ goes ``the other way'' from $\R^n$ to $\R^m$.
\end{question}


\begin{definition}
  If $L:\R^m \to \R^n$ is a linear map, the \textbf{adjoint} of $L$ is
  the map $L_{B_L^*} : \R^m \to \R^n$.  We usually write $L^*$ for the
  adjoint of $L$.
\end{definition}
		
Let $L: \R^m \to \R^n$ be a linear map.  
Show that $\langle \vec{v}, L(\vec{w})\rangle = \langle  L^*(\vec{v}),\vec{w}\rangle$ for every $v \in \R^n$ and $w \in \R^m$
\begin{free-response}
  \begin{align*}
    \langle \vec{v}, L(\vec{w})\rangle &= B_L(\vec{v},\vec{w})\\
    &= B_L^*(\vec{w},\vec{v})\\
    &=\langle \vec{w},L^*(\vec{v}) \rangle\\
    &=\langle L^*(\vec{v}), \vec{w}\rangle
  \end{align*}
\end{free-response}

\begin{question}
  Let's work through an example.  Let $L:\R^3 \to \R^2$ be the linear
  map represented by the matrix \(\begin{bmatrix} 3&2 &1\\
    -4&2&9\end{bmatrix}\) with respect to the standard basis.

  \begin{solution}
    \begin{hint}
      $L(\vec{e}_1) = \begin{bmatrix} 3 \\ -4 \end{bmatrix}$
    \end{hint}
    \begin{hint}
      $\langle \vec{e}_2, L(\vec{e}_1) \rangle = \langle \vec{e}_2, \begin{bmatrix} 3 \\ -4 \end{bmatrix} \rangle$
    \end{hint}
    \begin{hint}
      $\langle \vec{e}_2, \begin{bmatrix} 3 \\ -4 \end{bmatrix} \rangle = -4$.
    \end{hint}

    $\langle \vec{e}_2, L(\vec{e}_1) \rangle = $ \answer{$-4$}
  \end{solution}

  Recall that $\langle \vec{v}, L(\vec{w})\rangle = \langle  L^*(\vec{v}),\vec{w}\rangle$.  Consequently, setting $\vec{v} = \vec{e}_2$ and $\vec{w} = \vec{e}_1$, we have $\langle L^*(\vec{e}_2), \vec{e}_1 \rangle$ is also $-4$.

  Let's write $(\ell_{ij})$ for the entries of the matrix for $L$, and
  $(\ell^*_{ij})$ for the entries of the matrix for $L^*$.  The fact
  that $\langle \vec{e}_2, L(\vec{e}_1) \rangle = -4$ amounts to
  saying $\ell_{2,1} = -4$, and then since $\langle L^*(\vec{e}_1), \vec{e}_2 \rangle = -4$, we have that $\ell^*_{1,2} = -4$.

  \begin{solution}
    \begin{hint}
      The matrix of the adjoint of a linear map is the transpose of the matrix of that linear map.
    \end{hint}
    \begin{hint}
      The matrix of $L^*$ is \(\begin{bmatrix} 3&-2\\ 2&2\\1&9\end{bmatrix}\)
    \end{hint}
    What is the matrix of $L^*$?
    \begin{matrix-answer}
      correctMatrix = [['3','-4'],['2','2'],['1','9']]
    \end{matrix-answer}
  \end{solution}

  What do you notice about these entries?
  \begin{solution}
    \begin{multiple-choice}
      \choice[correct]{$\ell_{ij} = \ell^*_{ji}$}
      \choice{$\ell_{ij} = \ell^*_{ij}$}
    \end{multiple-choice}
  \end{solution}
  
  Let's summarize this fact as a theorem.
  \begin{theorem}
    Let $L: \R^n \to \R^m$ be a linear map.
    If $L$ has matrix $M$ with respect to the standard basis, then $L^*$ has matrix $M^\top$.    
  \end{theorem}
  
  Recall that $M^\top$ is the \textbf{transpose} of $M$, meaning that $M_{ij} = M^\top_{ij}$.
  
  It is your turn to prove this theorem.
  
  \begin{free-response}
    Let's use the fundamental insight from \href{http://ximera.osu.edu/course/kisonecat/m2o2c2/course/activity/week1/inner-product/multiply-dot/}{this activity}.  
    
    Let the matrix of $L^*$ be called $M^*$ for now.
    To find the entry in the $i^{th}$ row and $j^{th}$ column of $M^*$, we just  compute 
    \begin{align*}
      M_{i,j} &= e_i^\top M^*(e_j)\\
      &=e_i^\top L^*(e_j)\\
      &= \langle e_i , L^*(e_j)\rangle\\
      &= \langle  L(e_i), e_j \rangle\\
      &=e_j^\top L(e_i)\\
      &=e_j^\top M(e_i)\\
      &= M_{j,i}
    \end{align*}
    
    So the entry  in the $i^{th}$ row and $j^{th}$ column of $M^*$ is the entry in the $j^{th}$ row and $i^{th}$ column of $M$.  Thus $M^* = M^\top$.
  \end{free-response}
  
  Here, finally, is a question for you to ponder: \textit{why are we bothering about adjoints of linear maps if we have transposes of matrices?}
  
\end{question}

	
\end{document}