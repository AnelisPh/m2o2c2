\documentclass{ximera}
\title{Adjoints}

\begin{document}
	\begin{abstract}
		Adjoints formalize the transpose
	\end{abstract}
	
		Let $L: \R^m \to \R^n$ be a linear map.  
		Then there is an associated bilinear form $B_L  : \R^n \times \R^m \to \R$ given by $B_L(\vec{v},\vec{w}) = \langle \vec{v}, L(\vec{w}) \rangle$
		Now we can just as easily view $B_L$ as a bilinear map  $B_L^*: \R^m \times \R^n \to \R$, namely $B_L^* (\vec{w},\vec{v}) = B_L(\vec{v},\vec{w})$.
		But then we get an associated linear map $L_{B_L^*} : \R^n \to \R^m$.
		
		\begin{definition}
			If $L:\R^m \to \R^n$ is a linear map, we call the associated map $L^* : \R^m \to \R^n$ given by the line of reasoning above the \textbf{adjoint} of $L$.
		\end{definition}
		
			Let $L: \R^m \to \R^n$ be a linear map.  
			Show that $\langle \vec{v}, L(\vec{w})\rangle = \langle  L^*(\vec{v}),\vec{w}\rangle$ for every $v \in \R^n$ and $w \in \R^m$
			\begin{free-response}
				\begin{align*}
					\langle \vec{v}, L(\vec{w})\rangle &= B_L(\vec{v},\vec{w})\\
						&= B_L^*(\vec{w},\vec{v})\\
						&=\langle \vec{w},L^*(\vec{v}) \rangle\\
						&=\langle L^*(\vec{v}), \vec{w}\rangle
				\end{align*}
			\end{free-response}
	
			Let $L: \R^n \to \R^m$ be a linear map.
			If $L$ has matrix $M$ with respect to the standard basis, show that $L^*$ has matrix $M^\top$.
\begin{free-response}
	Let's use the fundamental insight from \href{http://ximera.osu.edu/course/kisonecat/m2o2c2/course/activity/week1/inner-product/multiply-dot/}{this activity}.  
	
	Let the matrix of $L^*$ be called $M^*$ for now.
	To find the entry in the $i^{th}$ row and $j^{th}$ column of $M^*$, we just  compute 
	\begin{align*}
		M_{i,j} &= e_i^\top M^*(e_j)\\
			&=e_i^\top L^*(e_j)\\
			&= \langle e_i , L^*(e_j)\rangle\\
			&= \langle  L(e_i), e_j \rangle\\
			&=e_j^\top L(e_i)\\
			&=e_j^\top M(e_i)\\
			&= M_{j,i}
	\end{align*}
	
	So the entry  in the $i^{th}$ row and $j^{th}$ column of $M^*$ is the entry in the $j^{th}$ row and $i^{th}$ column of $M$.  Thus $M^* = M^\top$.
\end{free-response}

\begin{question}
	Let $L:\R^3 \to \R^2$ have the matrix \(\begin{bmatrix} 3&2 &1\\ -4&2&9\end{bmatrix}\).
	\begin{solution}
		\begin{hint}
			The matrix of the adjoint of a linear map is the transpose of the matrix of that linear map.
		\end{hint}
		\begin{hint}
			The matrix of $L^*$ is \(\begin{bmatrix} 3&-2\\ 2&2\\1&9\end{bmatrix}\)
		\end{hint}
		What is the matrix of $L^*$?
			\begin{matrix-answer}
				correctMatrix = [['3','-4'],['2','2'],['1','9']]
			\end{matrix-answer}
	\end{solution}
\end{question}

		
			Show that if $\vec{v}$ is a nonzero eigenvector of $L:\R^n \to \R^n$, with eigenvalue $\lambda$ then
			there is a nonzero eigenvector $\vec{u}$ of $L^*$ with eigenvector $\lambda$. 
\begin{warning}
	This is a very hard problem.
\end{warning}
\begin{hint}
	Consider the map $S: \R^n \to \R^n$ given by $S(\vec{v}) = L^*(\vec{v}) - \lambda \vec{v}$, or in other words $S = L^* - \lambda I$.  Showing that this map has a nontrivial kernel is the same as
	showing that $L^*$ has $\lambda$ as an eigenvector.
\end{hint}
\begin{hint}
	Notice that $L-\lambda I$ is adjoint to $S = L^* -\lambda I$
\end{hint}
\begin{hint}
	For all $\vec{w} \in \R^n$, we have $\langle S(\vec{w}),\vec{v}\rangle = \langle \vec{w}, L(\vec{v}) - \lambda \vec{v}\rangle = 0$
\end{hint}
\begin{hint}
	Thus $S(\vec{w})$ is in the subspace of vectors perpendicular to the eigenvector $\vec{v}$, which we denote $\vec{v}^\perp$.
\end{hint}
\begin{hint}
	Thus we have that $Im(S) \subset \vec{v}^\perp$.  This implies that $dim(Im(S)) \leq n-1$
\end{hint}
\begin{hint}
	By the rank nullity theorem, we have that $dim(ker(S)) \geq 1$
\end{hint}
\begin{hint}
	So $S$ has a nontrivial kernel, so $L^*$ has a nonzero eigenvector $\vec{u}$ with eigenvalue $\lambda$. 
\end{hint}
\begin{free-response}
	Consider the map $S: \R^n \to \R^n$ given by $S(\vec{v}) = L^*(\vec{v}) - \lambda \vec{v}$.  Showing that this map has a nontrivial kernel is the same as
	showing that $L^*$ has $\lambda$ as an eigenvector.
	
	Notice that $L-\lambda I$ is adjoint to $S = L^* -\lambda I$
	
	For all $\vec{w} \in \R^n$, we have $\langle S(\vec{w}),\vec{v}\rangle = \langle \vec{w}, L(\vec{v}) - \lambda \vec{v}\rangle = 0$
	
	Thus $S(\vec{w})$ is in the subspace of vectors perpendicular to the eigenvector $\vec{v}$, which we denote $\vec{v}^\perp$.
	
	Thus we have that $Im(S) \subset \vec{v}^\perp$.  This implies that $dim(Im(S)) \leq n-1$
	
	By the rank nullity theorem, we have that $dim(ker(S)) \geq 1$
	
	So $S$ has a nontrivial kernel, so $L^*$ has a nonzero eigenvector $\vec{u}$ with eigenvalue $\lambda$. 
\end{free-response}
		
		\begin{definition}
			A linear operator $L:\R^n \to \R^n$ is called \textit{self adjoint} if $L = L^*$.
		\end{definition}
		
		\begin{definition}
			The matrix of a self-adjoint operator will be equal to its own transpose.  Thus it is symmetric about the main diagonal.  We call such matrices \textbf{symmetric matrices}.
		\end{definition}

\begin{definition}
	A bilinear form on $\R^n$ is \textit{symmetric} if $B(\vec{v},\vec{w}) = B(\vec{w},\vec{v})$ for all $\vec{v},\vec{w} \in \R^n$.
\end{definition}

\begin{example}
	Show that the dot product on $\R^n$ is a symmetric bilinear form, since we have already shown that $v \cdot w = w \cdot v$
\end{example}

\begin{question}
	Which of the following bilinear forms on $\R^2$ are symmetric?
		\begin{solution}
			\begin{hint}
				$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = x_1y_2+3x_2y_1$ is not symmetric since, for example,
				$B\left( \verticalvector{1\\0},\verticalvector{0\\1}\right) = 1$, but $B\left( \verticalvector{0\\1},\verticalvector{1\\0}\right) = 3$.
			\end{hint}
			\begin{hint}
				$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = x_1y_1+5x_2y_2+x_1y_2$ is not symmetric since, for example,
				$B\left(\verticalvector{1,0},\verticalvector{0,1}\right) = 1$ but  $B\left( \verticalvector{0\\1},\verticalvector{1\\0}\right) = 0$
			\end{hint}
			\begin{hint}
				$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = 2x_1y_1+4x_2y_2$ is symmetric, since
				$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = 2x_1y_1+4x_2y_2 = B\left(\verticalvector{y_1,y_2}, \verticalvector{x_1,x_2}\right) $
			\end{hint}
			\begin{multiple-choice}
				\choice{$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = x_1y_2+3x_2y_1$}
				\choice[correct]{$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = 2x_1y_1+4x_2y_2$}
				\choice{$B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = x_1y_1+5x_2y_2+x_1y_2$}				
			\end{multiple-choice}
		\end{solution}
\end{question}

%\begin{question}
%	If $B$ is a symmetric bilinear form on $\R^3$, and (BADBAD give action of B on half of pairs of basis vectors), then $B((x_1,x_2,x_3),(y_1,y_2,y_3)) = $?
%\end{question}

\begin{question}
	Let $B$ be the symmetric bilinear form on $\R^2$ defined by $B\left(\verticalvector{x_1,x_2},\verticalvector{y_1,y_2}\right) = 2x_1y_1+4x_2y_2+x_1y_2+x_2y_1$.  What is the matrix of $B$?  What do you notice about this matrix?
	\begin{solution}
		\begin{hint}
			Remember that the entry $M_{i,j} = B(e_i,e_j)$
		\end{hint}
		\begin{hint}
			\begin{align*}
				M_{1,1} = B(\verticalvector{1\\0},\verticalvector{1\\0}) = 2\\
				M_{1,2} = B(\verticalvector{1\\0},\verticalvector{0\\1}) = 1\\
				M_{2,1} = B(\verticalvector{0\\1},\verticalvector{1\\0}) = 1\\
				M_{2,2} = M(\verticalvector{0\\1},\verticalvector{0\\1}) = 4
			\end{align*}
		\end{hint}
		\begin{hint}
			The matrix of $B$ is \(\begin{bmatrix} 2 & 1 \\ 1 & 4\end{bmatrix}\)
		\end{hint}
		\begin{matrix-answer}
			correctMatrix = [['2','1'],['1','4']]
		\end{matrix-answer}
	\end{solution}
	Notice that this matrix is a symmetric matrix!
\end{question}

	Show that $L$ is self adjoint if and only if the bilinear form associated to it is symmetric.
\begin{free-response}
	If $L$ is self adjoint, then 
	\begin{align*}
	B_L(v,w) = v^\top L(w) &= \langle v , L(w)\rangle\\
		&= \langle L(v), w \rangle\\
		&= w^\top L(v)\\
		&=B_L(w,v)
	\end{align*}
	
	So the bilinear form associated to $L$ is symmetric.  On the other hand, if $B$ is a symmetric, then
		\begin{align*}
		\langle L_B(v) , w \rangle &= \langle  B(v,\cdot)^top, w\rangle\\
			&= B(v,\cdot)(w)\\
			&=B(v,w)\\
			&=B(w,v)\\
			&=B(w,\cdot)(v)\\
			&=\langle B(w,\cdot)^\top , v \rangle
			&=\langle L_B(w), v\rangle
		\end{align*}
		
		So $L_B$ is self-adjoint
\end{free-response}
	
\end{document}